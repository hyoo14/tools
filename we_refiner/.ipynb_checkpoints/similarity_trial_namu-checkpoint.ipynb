{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# _*_ coding: UTF-8 _*_\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###코퍼스에 따라 주석체크###\n",
    "#corpus_filenm = \"text8.txt\"\n",
    "corpus_filenm = \"morphs_namu_small.txt\"\n",
    "# corpus_filenm = \"/hdd/data/namu-wiki/morphs_namu_small.txtad\"\n",
    "\n",
    "train_mode = True\n",
    "# train_mode = False\n",
    "\n",
    "batch_size = 128            # Number of examples in a mini-batch\n",
    "embedding_size = 128        # Dimension of the embedding vector.\n",
    "skip_window = 2             # How many words to consider left and right.\n",
    "num_neg_sampled = 64        # Number of negative examples to sample.\n",
    "vocabulary_size = 50000     # Size of vocabulary\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors.\n",
    "valid_size = 16\n",
    "valid_window = 1000\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "###코퍼스에 따라 주석체크###\n",
    "#num_steps = 100000 #text8\n",
    "num_steps = 1000000 #morphs_namu_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 154458803\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Read corpus and tokenize words based on space.\n",
    "def read_data(filename):\n",
    "    with open(filename, \"r\", encoding='UTF8') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "        return ' '.join(lines)\n",
    "\n",
    "text = read_data(corpus_filenm)\n",
    "text_words = text.split()\n",
    "data_size = len(text_words)\n",
    "print('Data size', data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word coverage: 98.22%\n",
      "Most common words (+UNK) [('UNK', 0), ('하', 6135340), ('이', 5212869), ('는', 3644212), ('ㄴ', 3488758)]\n",
      "Sample data [8019, 11, 743, 1, 3, 119, 27, 5, 5554, 5] ['새우', '를', '사랑', '하', '는', '사람', '들', '의', '모임', '의']\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Build vocabulary using most common top k words, replace word with ids in the corpus.\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"\n",
    "    :param words: list of all words in corpus\n",
    "    :param n_words: vocabulary size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    unique = collections.Counter(words)     # python dict - key(word): value(freq)\n",
    "    orders = unique.most_common(n_words - 1)\n",
    "    count = [('UNK', 0)]\n",
    "    count.extend(orders)\n",
    "\n",
    "    # check vocabulary coverage\n",
    "    total_freq = 0\n",
    "    for word, freq in orders:\n",
    "        total_freq += freq\n",
    "    print(\"word coverage: %.2f%%\" % (100.0 * total_freq / data_size))  # word coverage\n",
    "\n",
    "    # build word2id dictionary and id2word reverse dictionary.\n",
    "    word2id = dict()\n",
    "    for word, _ in count:\n",
    "        word2id[word] = len(word2id)\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "    # build training data by replacing all words with word ids.\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        # if the word is not in the dictionary, index will be 0. (i.e. 'UNK')\n",
    "        index = word2id.get(word, 0)\n",
    "        data.append(index)\n",
    "\n",
    "    return data, count, word2id, id2word\n",
    "\n",
    "vocab_counts = []\n",
    "data, count, word2id, id2word = build_dataset(text_words, vocabulary_size)\n",
    "\n",
    "\n",
    "del text_words\n",
    "\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [id2word[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3. 학습에 사용할 mini-batch 생성\n",
    "def generate_batch(batch_size, skip_window):\n",
    "    \n",
    "    global data_index\n",
    "    \n",
    "    num_targets = skip_window * 2\n",
    "    \n",
    "    assert batch_size % num_targets == 0\n",
    "    assert num_targets <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=batch_size, dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 \n",
    "    buffer = collections.deque(maxlen=span)##########buffer에 문제가 있음 deque error가 계속 뜬다..\n",
    "    \n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "        \n",
    "        \n",
    "    #buffer.append(data[data_index: data_index + span])\n",
    "    #data_index += span\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size // num_targets):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "\n",
    "        for j, context_word in enumerate(context_words):\n",
    "            batch[i * num_targets + j ] = buffer[skip_window]#buffer[0][skip_window]        # center words  e.g. [2, 2, 2, 2]   #ㄹㅇ..?\n",
    "            labels[i * num_targets + j, 0] = buffer[context_word]#buffer[0][context_word]   # context words e.g. [0, 1, 3, 4]\n",
    "\n",
    "        if data_index == len(data):\n",
    "            # reset data index\n",
    "            for word in data[:span]:\n",
    "                buffer.append(word)\n",
    "        else:\n",
    "            # adding words to buffer\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743 사랑 -> 8019 새우\n",
      "743 사랑 -> 11 를\n",
      "743 사랑 -> 1 하\n",
      "743 사랑 -> 3 는\n",
      "1 하 -> 11 를\n",
      "1 하 -> 743 사랑\n",
      "1 하 -> 3 는\n",
      "1 하 -> 119 사람\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "batch, labels = generate_batch(batch_size=8, skip_window=skip_window)\n",
    "for i in range(8):\n",
    "    print(batch[i], id2word[batch[i]],\n",
    "          '->', labels[i, 0], id2word[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity :  Tensor(\"MatMul_1:0\", shape=(16, 50000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build a skip-gram tensorflow graph.\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)         # word_ids of validation words\n",
    "\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Important - convert word ids to embedding vectors\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss - NCE: Noise Contrasive Estimation\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    # http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_neg_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer: set up a variable that's incremented once per batch and controls the learning rate decay.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.1,            # Base learning rate.\n",
    "        global_step,    # Current index into the dataset.\n",
    "        num_steps,      # Decay step.\n",
    "        0.95,           # Decay rate.\n",
    "        staircase=True)\n",
    "\n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute the cosine similarity between mini-batch examples and all embeddings\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "\n",
    "    # validation dataset의 유사 단어 찾기\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    print('similarity : ',similarity)\n",
    "    # Add variable initializer\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  274.187133789\n",
      "Average loss at step  2000 :  142.884439646\n",
      "Average loss at step  4000 :  77.6042686119\n",
      "Average loss at step  6000 :  53.7004747543\n",
      "Average loss at step  8000 :  42.0602016876\n",
      "Average loss at step  10000 :  35.4951064072\n",
      "Average loss at step  12000 :  28.7186021571\n",
      "Average loss at step  14000 :  25.8240352874\n",
      "Average loss at step  16000 :  20.673102509\n",
      "Average loss at step  18000 :  18.8482707546\n",
      "Average loss at step  20000 :  16.5488815451\n",
      "Average loss at step  22000 :  15.9561400299\n",
      "Average loss at step  24000 :  15.4246680901\n",
      "Average loss at step  26000 :  14.9248738786\n",
      "Average loss at step  28000 :  12.7247613397\n",
      "Average loss at step  30000 :  12.8401871598\n",
      "Average loss at step  32000 :  12.0201301928\n",
      "Average loss at step  34000 :  11.6273514855\n",
      "Average loss at step  36000 :  11.3125922432\n",
      "Average loss at step  38000 :  9.79359254289\n",
      "Average loss at step  40000 :  9.40264797044\n",
      "Average loss at step  42000 :  9.24118405104\n",
      "Average loss at step  44000 :  9.77039698291\n",
      "Average loss at step  46000 :  8.58431072688\n",
      "Average loss at step  48000 :  8.69471785104\n",
      "Average loss at step  50000 :  8.39000711608\n",
      "Average loss at step  52000 :  8.84718369657\n",
      "Average loss at step  54000 :  8.07844003737\n",
      "Average loss at step  56000 :  7.73710945189\n",
      "Average loss at step  58000 :  8.16983832979\n",
      "Average loss at step  60000 :  8.0807155242\n",
      "Average loss at step  62000 :  7.73753287101\n",
      "Average loss at step  64000 :  8.44890787613\n",
      "Average loss at step  66000 :  7.46371968532\n",
      "Average loss at step  68000 :  7.38738212883\n",
      "Average loss at step  70000 :  7.60538581586\n",
      "Average loss at step  72000 :  7.05285310948\n",
      "Average loss at step  74000 :  7.28051655471\n",
      "Average loss at step  76000 :  7.32301687455\n",
      "Average loss at step  78000 :  7.07001501369\n",
      "Average loss at step  80000 :  7.33228880692\n",
      "Average loss at step  82000 :  7.29941098499\n",
      "Average loss at step  84000 :  7.0247300477\n",
      "Average loss at step  86000 :  7.04457032216\n",
      "Average loss at step  88000 :  6.44503845167\n",
      "Average loss at step  90000 :  6.63260017103\n",
      "Average loss at step  92000 :  6.20368972719\n",
      "Average loss at step  94000 :  6.68607785273\n",
      "Average loss at step  96000 :  6.52847405577\n",
      "Average loss at step  98000 :  6.57372129714\n",
      "Average loss at step  100000 :  6.42839893818\n",
      "Average loss at step  102000 :  6.54976586926\n",
      "Average loss at step  104000 :  6.72197349977\n",
      "Average loss at step  106000 :  6.52578791499\n",
      "Average loss at step  108000 :  6.36116646242\n",
      "Average loss at step  110000 :  6.04781090605\n",
      "Average loss at step  112000 :  6.53865387917\n",
      "Average loss at step  114000 :  6.556689255\n",
      "Average loss at step  116000 :  6.39093211341\n",
      "Average loss at step  118000 :  6.08586005497\n",
      "Average loss at step  120000 :  6.1505599016\n",
      "Average loss at step  122000 :  5.09232597947\n",
      "Average loss at step  124000 :  6.10720745683\n",
      "Average loss at step  126000 :  6.09824466848\n",
      "Average loss at step  128000 :  6.09540692198\n",
      "Average loss at step  130000 :  5.99585133147\n",
      "Average loss at step  132000 :  5.9977910341\n",
      "Average loss at step  134000 :  6.36401144278\n",
      "Average loss at step  136000 :  6.51303176558\n",
      "Average loss at step  138000 :  6.07333395672\n",
      "Average loss at step  140000 :  6.08258047163\n",
      "Average loss at step  142000 :  5.99194906378\n",
      "Average loss at step  144000 :  5.69230667996\n",
      "Average loss at step  146000 :  6.31069202042\n",
      "Average loss at step  148000 :  5.66283251524\n",
      "Average loss at step  150000 :  5.90373577023\n",
      "Average loss at step  152000 :  6.34021679139\n",
      "Average loss at step  154000 :  5.74737262976\n",
      "Average loss at step  156000 :  5.86535553646\n",
      "Average loss at step  158000 :  5.74287240171\n",
      "Average loss at step  160000 :  5.82315395844\n",
      "Average loss at step  162000 :  6.10058280778\n",
      "Average loss at step  164000 :  5.63528965795\n",
      "Average loss at step  166000 :  5.88127985287\n",
      "Average loss at step  168000 :  5.94213056052\n",
      "Average loss at step  170000 :  5.8061833266\n",
      "Average loss at step  172000 :  5.91434350371\n",
      "Average loss at step  174000 :  5.74871410632\n",
      "Average loss at step  176000 :  5.65818933892\n",
      "Average loss at step  178000 :  5.80161273575\n",
      "Average loss at step  180000 :  5.71212570071\n",
      "Average loss at step  182000 :  5.78003142118\n",
      "Average loss at step  184000 :  6.13696539581\n",
      "Average loss at step  186000 :  6.19133113778\n",
      "Average loss at step  188000 :  5.50991473907\n",
      "Average loss at step  190000 :  6.02601045358\n",
      "Average loss at step  192000 :  5.84833434331\n",
      "Average loss at step  194000 :  5.79099965978\n",
      "Average loss at step  196000 :  5.74500691879\n",
      "Average loss at step  198000 :  5.64122314113\n",
      "Average loss at step  200000 :  5.58726872718\n",
      "Average loss at step  202000 :  5.95676103091\n",
      "Average loss at step  204000 :  5.25639864457\n",
      "Average loss at step  206000 :  5.08476156986\n",
      "Average loss at step  208000 :  5.52716780412\n",
      "Average loss at step  210000 :  5.07800924873\n",
      "Average loss at step  212000 :  5.26620748997\n",
      "Average loss at step  214000 :  5.23273102653\n",
      "Average loss at step  216000 :  5.47765815401\n",
      "Average loss at step  218000 :  5.21514438993\n",
      "Average loss at step  220000 :  5.6832354387\n",
      "Average loss at step  222000 :  5.68437913489\n",
      "Average loss at step  224000 :  5.5398211472\n",
      "Average loss at step  226000 :  5.40603585899\n",
      "Average loss at step  228000 :  5.44876631904\n",
      "Average loss at step  230000 :  5.35854796505\n",
      "Average loss at step  232000 :  5.60663569093\n",
      "Average loss at step  234000 :  4.86865715933\n",
      "Average loss at step  236000 :  4.60942893732\n",
      "Average loss at step  238000 :  4.98532774317\n",
      "Average loss at step  240000 :  5.50137911797\n",
      "Average loss at step  242000 :  5.9383480649\n",
      "Average loss at step  244000 :  5.40839652526\n",
      "Average loss at step  246000 :  5.43114242578\n",
      "Average loss at step  248000 :  5.18100711882\n",
      "Average loss at step  250000 :  5.42082737589\n",
      "Average loss at step  252000 :  5.29734666646\n",
      "Average loss at step  254000 :  5.26692365181\n",
      "Average loss at step  256000 :  5.50196167934\n",
      "Average loss at step  258000 :  5.0686411382\n",
      "Average loss at step  260000 :  5.54714053118\n",
      "Average loss at step  262000 :  5.46403528142\n",
      "Average loss at step  264000 :  5.79727989507\n",
      "Average loss at step  266000 :  5.45347964084\n",
      "Average loss at step  268000 :  5.59401469874\n",
      "Average loss at step  270000 :  5.01249158514\n",
      "Average loss at step  272000 :  5.55755129254\n",
      "Average loss at step  274000 :  5.6884501884\n",
      "Average loss at step  276000 :  4.77701468182\n",
      "Average loss at step  278000 :  4.92041196495\n",
      "Average loss at step  280000 :  5.36830961645\n",
      "Average loss at step  282000 :  5.61240364087\n",
      "Average loss at step  284000 :  5.44127591455\n",
      "Average loss at step  286000 :  5.36328382075\n",
      "Average loss at step  288000 :  5.32281263411\n",
      "Average loss at step  290000 :  5.34462466955\n",
      "Average loss at step  292000 :  5.43698636234\n",
      "Average loss at step  294000 :  5.33463715005\n",
      "Average loss at step  296000 :  5.03739104784\n",
      "Average loss at step  298000 :  5.62238906789\n",
      "Average loss at step  300000 :  5.59642138743\n",
      "Average loss at step  302000 :  5.81326086533\n",
      "Average loss at step  304000 :  5.4105219202\n",
      "Average loss at step  306000 :  5.40303924954\n",
      "Average loss at step  308000 :  5.86411349237\n",
      "Average loss at step  310000 :  5.33415728307\n",
      "Average loss at step  312000 :  5.30409796226\n",
      "Average loss at step  314000 :  5.5232771548\n",
      "Average loss at step  316000 :  5.21871280575\n",
      "Average loss at step  318000 :  5.06994982219\n",
      "Average loss at step  320000 :  5.2537147156\n",
      "Average loss at step  322000 :  5.26684200597\n",
      "Average loss at step  324000 :  5.40050015676\n",
      "Average loss at step  326000 :  5.22811208785\n",
      "Average loss at step  328000 :  5.31199648035\n",
      "Average loss at step  330000 :  5.35347927856\n",
      "Average loss at step  332000 :  5.36318849969\n",
      "Average loss at step  334000 :  5.5770511961\n",
      "Average loss at step  336000 :  5.46199110454\n",
      "Average loss at step  338000 :  5.50769629729\n",
      "Average loss at step  340000 :  5.33356019175\n",
      "Average loss at step  342000 :  4.66036170316\n",
      "Average loss at step  344000 :  5.64656536114\n",
      "Average loss at step  346000 :  5.36112395978\n",
      "Average loss at step  348000 :  5.08586759245\n",
      "Average loss at step  350000 :  4.91008505234\n",
      "Average loss at step  352000 :  4.90729541594\n",
      "Average loss at step  354000 :  5.27810411441\n",
      "Average loss at step  356000 :  5.25012744439\n",
      "Average loss at step  358000 :  4.80162780726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  360000 :  5.39332402182\n",
      "Average loss at step  362000 :  5.35398185492\n",
      "Average loss at step  364000 :  4.83301894283\n",
      "Average loss at step  366000 :  5.204488644\n",
      "Average loss at step  368000 :  4.81731287074\n",
      "Average loss at step  370000 :  5.20227077788\n",
      "Average loss at step  372000 :  5.0758369804\n",
      "Average loss at step  374000 :  5.07633251935\n",
      "Average loss at step  376000 :  5.13124950033\n",
      "Average loss at step  378000 :  4.97579801571\n",
      "Average loss at step  380000 :  5.25484961551\n",
      "Average loss at step  382000 :  5.34168158615\n",
      "Average loss at step  384000 :  5.15330231082\n",
      "Average loss at step  386000 :  5.46305644536\n",
      "Average loss at step  388000 :  5.31834293365\n",
      "Average loss at step  390000 :  5.08493613267\n",
      "Average loss at step  392000 :  4.95770838356\n",
      "Average loss at step  394000 :  5.32476132452\n",
      "Average loss at step  396000 :  5.20950575851\n",
      "Average loss at step  398000 :  5.02322505927\n",
      "Average loss at step  400000 :  5.3805464381\n",
      "Average loss at step  402000 :  5.13213471401\n",
      "Average loss at step  404000 :  5.39230402815\n",
      "Average loss at step  406000 :  5.25932367671\n",
      "Average loss at step  408000 :  5.04539020848\n",
      "Average loss at step  410000 :  5.34930469108\n",
      "Average loss at step  412000 :  5.01972156841\n",
      "Average loss at step  414000 :  5.72171996665\n",
      "Average loss at step  416000 :  5.2475465405\n",
      "Average loss at step  418000 :  5.15155988824\n",
      "Average loss at step  420000 :  5.31308563447\n",
      "Average loss at step  422000 :  5.07863736665\n",
      "Average loss at step  424000 :  5.10573025292\n",
      "Average loss at step  426000 :  4.94522544837\n",
      "Average loss at step  428000 :  5.23298488939\n",
      "Average loss at step  430000 :  5.03902846169\n",
      "Average loss at step  432000 :  5.24375468802\n",
      "Average loss at step  434000 :  5.15001138103\n",
      "Average loss at step  436000 :  5.30225848186\n",
      "Average loss at step  438000 :  5.0089535315\n",
      "Average loss at step  440000 :  5.21526769233\n",
      "Average loss at step  442000 :  5.15650214821\n",
      "Average loss at step  444000 :  5.00057445624\n",
      "Average loss at step  446000 :  5.15067235112\n",
      "Average loss at step  448000 :  4.52200893915\n",
      "Average loss at step  450000 :  5.34893770349\n",
      "Average loss at step  452000 :  4.79290419912\n",
      "Average loss at step  454000 :  4.87371186721\n",
      "Average loss at step  456000 :  5.38284880483\n",
      "Average loss at step  458000 :  5.15078199875\n",
      "Average loss at step  460000 :  4.99194744015\n",
      "Average loss at step  462000 :  4.86019430208\n",
      "Average loss at step  464000 :  5.10273698997\n",
      "Average loss at step  466000 :  5.20818177903\n",
      "Average loss at step  468000 :  5.00599280608\n",
      "Average loss at step  470000 :  5.15660487545\n",
      "Average loss at step  472000 :  5.16976471901\n",
      "Average loss at step  474000 :  5.1458948884\n",
      "Average loss at step  476000 :  4.94872492254\n",
      "Average loss at step  478000 :  5.04735160363\n",
      "Average loss at step  480000 :  4.96227324128\n",
      "Average loss at step  482000 :  5.26392920518\n",
      "Average loss at step  484000 :  5.01205815172\n",
      "Average loss at step  486000 :  4.9805271517\n",
      "Average loss at step  488000 :  5.35131429124\n",
      "Average loss at step  490000 :  4.83486235344\n",
      "Average loss at step  492000 :  4.65032021761\n",
      "Average loss at step  494000 :  5.29225256002\n",
      "Average loss at step  496000 :  5.16658072734\n",
      "Average loss at step  498000 :  5.21362502563\n",
      "Average loss at step  500000 :  5.12994668102\n",
      "Average loss at step  502000 :  4.99371697021\n",
      "Average loss at step  504000 :  5.36458724403\n",
      "Average loss at step  506000 :  5.08179861641\n",
      "Average loss at step  508000 :  5.22177046072\n",
      "Average loss at step  510000 :  4.87662633228\n",
      "Average loss at step  512000 :  5.11936615986\n",
      "Average loss at step  514000 :  5.20760947192\n",
      "Average loss at step  516000 :  5.14922516191\n",
      "Average loss at step  518000 :  4.86168986833\n",
      "Average loss at step  520000 :  5.15747408104\n",
      "Average loss at step  522000 :  5.18765417182\n",
      "Average loss at step  524000 :  5.02667243958\n",
      "Average loss at step  526000 :  5.22245053875\n",
      "Average loss at step  528000 :  5.11937810957\n",
      "Average loss at step  530000 :  5.52678242636\n",
      "Average loss at step  532000 :  5.44809896553\n",
      "Average loss at step  534000 :  4.97362879038\n",
      "Average loss at step  536000 :  5.22126663542\n",
      "Average loss at step  538000 :  5.05795366776\n",
      "Average loss at step  540000 :  5.19090473568\n",
      "Average loss at step  542000 :  5.03964185661\n",
      "Average loss at step  544000 :  4.94469778204\n",
      "Average loss at step  546000 :  5.25984424174\n",
      "Average loss at step  548000 :  5.19450740767\n",
      "Average loss at step  550000 :  4.95540268183\n",
      "Average loss at step  552000 :  4.86568100071\n",
      "Average loss at step  554000 :  5.04364462042\n",
      "Average loss at step  556000 :  4.90388320053\n",
      "Average loss at step  558000 :  5.17076554525\n",
      "Average loss at step  560000 :  5.09497205174\n",
      "Average loss at step  562000 :  5.02749863064\n",
      "Average loss at step  564000 :  4.93008581126\n",
      "Average loss at step  566000 :  5.08187104034\n",
      "Average loss at step  568000 :  5.19068401599\n",
      "Average loss at step  570000 :  4.9712786411\n",
      "Average loss at step  572000 :  4.81909944332\n",
      "Average loss at step  574000 :  4.81277899909\n",
      "Average loss at step  576000 :  5.37930403429\n",
      "Average loss at step  578000 :  5.18164126146\n",
      "Average loss at step  580000 :  5.23440538919\n",
      "Average loss at step  582000 :  5.05677435875\n",
      "Average loss at step  584000 :  4.98573550069\n",
      "Average loss at step  586000 :  5.04005388057\n",
      "Average loss at step  588000 :  4.94332038796\n",
      "Average loss at step  590000 :  4.86907178807\n",
      "Average loss at step  592000 :  5.16439039302\n",
      "Average loss at step  594000 :  5.11828626025\n",
      "Average loss at step  596000 :  5.18899036312\n",
      "Average loss at step  598000 :  5.03349874038\n",
      "Average loss at step  600000 :  4.96923139071\n",
      "Average loss at step  602000 :  4.89948617929\n",
      "Average loss at step  604000 :  4.93702074134\n",
      "Average loss at step  606000 :  5.15791836464\n",
      "Average loss at step  608000 :  5.04074822223\n",
      "Average loss at step  610000 :  4.97062914288\n",
      "Average loss at step  612000 :  5.00575015318\n",
      "Average loss at step  614000 :  4.90149425828\n",
      "Average loss at step  616000 :  4.91621173644\n",
      "Average loss at step  618000 :  5.04497966802\n",
      "Average loss at step  620000 :  4.86834156859\n",
      "Average loss at step  622000 :  4.93893696702\n",
      "Average loss at step  624000 :  4.83505269241\n",
      "Average loss at step  626000 :  4.71296337807\n",
      "Average loss at step  628000 :  5.12799336374\n",
      "Average loss at step  630000 :  5.00320580482\n",
      "Average loss at step  632000 :  4.93268463314\n",
      "Average loss at step  634000 :  4.77236479998\n",
      "Average loss at step  636000 :  5.08840479875\n",
      "Average loss at step  638000 :  4.95938842428\n",
      "Average loss at step  640000 :  5.07761638653\n",
      "Average loss at step  642000 :  4.99743597865\n",
      "Average loss at step  644000 :  4.78871283078\n",
      "Average loss at step  646000 :  4.94849743927\n",
      "Average loss at step  648000 :  4.81507017493\n",
      "Average loss at step  650000 :  4.88798840594\n",
      "Average loss at step  652000 :  4.75161617911\n",
      "Average loss at step  654000 :  5.09532298315\n",
      "Average loss at step  656000 :  5.04502146578\n",
      "Average loss at step  658000 :  4.7712131027\n",
      "Average loss at step  660000 :  4.96520538038\n",
      "Average loss at step  662000 :  5.14177930593\n",
      "Average loss at step  664000 :  5.07509365046\n",
      "Average loss at step  666000 :  4.73592609227\n",
      "Average loss at step  668000 :  4.90687742627\n",
      "Average loss at step  670000 :  4.96997552752\n",
      "Average loss at step  672000 :  5.31965647924\n",
      "Average loss at step  674000 :  5.07216941977\n",
      "Average loss at step  676000 :  5.0868032279\n",
      "Average loss at step  678000 :  4.74146176398\n",
      "Average loss at step  680000 :  4.91145457256\n",
      "Average loss at step  682000 :  5.08092423391\n",
      "Average loss at step  684000 :  5.10539598712\n",
      "Average loss at step  686000 :  5.01842577028\n",
      "Average loss at step  688000 :  4.70029432666\n",
      "Average loss at step  690000 :  4.96092063516\n",
      "Average loss at step  692000 :  4.93233847165\n",
      "Average loss at step  694000 :  4.8323160001\n",
      "Average loss at step  696000 :  4.84351919293\n",
      "Average loss at step  698000 :  5.78343008983\n",
      "Average loss at step  700000 :  5.29499675834\n",
      "Average loss at step  702000 :  5.0812130124\n",
      "Average loss at step  704000 :  5.04036423409\n",
      "Average loss at step  706000 :  5.04565422726\n",
      "Average loss at step  708000 :  5.02935096991\n",
      "Average loss at step  710000 :  4.95629867911\n",
      "Average loss at step  712000 :  5.09924261796\n",
      "Average loss at step  714000 :  5.00897434831\n",
      "Average loss at step  716000 :  4.98774430943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  718000 :  4.90038175917\n",
      "Average loss at step  720000 :  5.14658292842\n",
      "Average loss at step  722000 :  5.00880983925\n",
      "Average loss at step  724000 :  5.09833860219\n",
      "Average loss at step  726000 :  5.14592354476\n",
      "Average loss at step  728000 :  5.02829241776\n",
      "Average loss at step  730000 :  4.79055783296\n",
      "Average loss at step  732000 :  5.18465112507\n",
      "Average loss at step  734000 :  4.92470217788\n",
      "Average loss at step  736000 :  5.06605354249\n",
      "Average loss at step  738000 :  5.03092074621\n",
      "Average loss at step  740000 :  4.91886341047\n",
      "Average loss at step  742000 :  5.09379223657\n",
      "Average loss at step  744000 :  5.34296619743\n",
      "Average loss at step  746000 :  5.05643007171\n",
      "Average loss at step  748000 :  4.97107665884\n",
      "Average loss at step  750000 :  4.94190826571\n",
      "Average loss at step  752000 :  4.74801280928\n",
      "Average loss at step  754000 :  4.94157555449\n",
      "Average loss at step  756000 :  4.95360238457\n",
      "Average loss at step  758000 :  5.14284550071\n",
      "Average loss at step  760000 :  5.06459493113\n",
      "Average loss at step  762000 :  4.99118884206\n",
      "Average loss at step  764000 :  5.04015755701\n",
      "Average loss at step  766000 :  5.0118225956\n",
      "Average loss at step  768000 :  4.91564095676\n",
      "Average loss at step  770000 :  4.99495778227\n",
      "Average loss at step  772000 :  4.91144784629\n",
      "Average loss at step  774000 :  5.00787008786\n",
      "Average loss at step  776000 :  4.93720503569\n",
      "Average loss at step  778000 :  4.78896461451\n",
      "Average loss at step  780000 :  4.2278248654\n",
      "Average loss at step  782000 :  4.13238977051\n",
      "Average loss at step  784000 :  4.83608839923\n",
      "Average loss at step  786000 :  4.93211576676\n",
      "Average loss at step  788000 :  4.99917248368\n",
      "Average loss at step  790000 :  4.86666970754\n",
      "Average loss at step  792000 :  5.08685814059\n",
      "Average loss at step  794000 :  4.99468610334\n",
      "Average loss at step  796000 :  4.79995483005\n",
      "Average loss at step  798000 :  4.89562676132\n",
      "Average loss at step  800000 :  4.88140861475\n",
      "Average loss at step  802000 :  4.85465715349\n",
      "Average loss at step  804000 :  5.02963529587\n",
      "Average loss at step  806000 :  4.84184708631\n",
      "Average loss at step  808000 :  4.86707142878\n",
      "Average loss at step  810000 :  4.88384149814\n",
      "Average loss at step  812000 :  4.86915539384\n",
      "Average loss at step  814000 :  4.92561573255\n",
      "Average loss at step  816000 :  4.93332228386\n",
      "Average loss at step  818000 :  4.9593364687\n",
      "Average loss at step  820000 :  4.92252023971\n",
      "Average loss at step  822000 :  4.79431332231\n",
      "Average loss at step  824000 :  4.80568713427\n",
      "Average loss at step  826000 :  4.8600562135\n",
      "Average loss at step  828000 :  4.9022378372\n",
      "Average loss at step  830000 :  4.67134404504\n",
      "Average loss at step  832000 :  5.16724212599\n",
      "Average loss at step  834000 :  5.026065328\n",
      "Average loss at step  836000 :  4.9370327087\n",
      "Average loss at step  838000 :  4.879228351\n",
      "Average loss at step  840000 :  4.97876224422\n",
      "Average loss at step  842000 :  4.7628350451\n",
      "Average loss at step  844000 :  4.42305842325\n",
      "Average loss at step  846000 :  3.56128392027\n",
      "Average loss at step  848000 :  4.92435299003\n",
      "Average loss at step  850000 :  4.78926593578\n",
      "Average loss at step  852000 :  4.91072860885\n",
      "Average loss at step  854000 :  4.94458543003\n",
      "Average loss at step  856000 :  4.93863170803\n",
      "Average loss at step  858000 :  4.67399983633\n",
      "Average loss at step  860000 :  4.94686144006\n",
      "Average loss at step  862000 :  5.05106537569\n",
      "Average loss at step  864000 :  4.66609774053\n",
      "Average loss at step  866000 :  4.88127763003\n",
      "Average loss at step  868000 :  4.90087628961\n",
      "Average loss at step  870000 :  4.93951263696\n",
      "Average loss at step  872000 :  4.91332530618\n",
      "Average loss at step  874000 :  4.48357843053\n",
      "Average loss at step  876000 :  4.88446279871\n",
      "Average loss at step  878000 :  4.85785484803\n",
      "Average loss at step  880000 :  5.01714238846\n",
      "Average loss at step  882000 :  5.01451244175\n",
      "Average loss at step  884000 :  4.92975211084\n",
      "Average loss at step  886000 :  4.83694234633\n",
      "Average loss at step  888000 :  4.68943601489\n",
      "Average loss at step  890000 :  4.86485873258\n",
      "Average loss at step  892000 :  4.79599075723\n",
      "Average loss at step  894000 :  4.88042991924\n",
      "Average loss at step  896000 :  4.80910248339\n",
      "Average loss at step  898000 :  4.80594164431\n",
      "Average loss at step  900000 :  4.94272687757\n",
      "Average loss at step  902000 :  5.02606047761\n",
      "Average loss at step  904000 :  4.87239531147\n",
      "Average loss at step  906000 :  4.81832842422\n",
      "Average loss at step  908000 :  4.96515865481\n",
      "Average loss at step  910000 :  4.80566983604\n",
      "Average loss at step  912000 :  5.03557603014\n",
      "Average loss at step  914000 :  5.14716952354\n",
      "Average loss at step  916000 :  4.75047656894\n",
      "Average loss at step  918000 :  5.00443246698\n",
      "Average loss at step  920000 :  4.96359687078\n",
      "Average loss at step  922000 :  4.92936274761\n",
      "Average loss at step  924000 :  4.80991529691\n",
      "Average loss at step  926000 :  4.85136401534\n",
      "Average loss at step  928000 :  4.79844717646\n",
      "Average loss at step  930000 :  4.67463148479\n",
      "Average loss at step  932000 :  4.91166533297\n",
      "Average loss at step  934000 :  4.92355389142\n",
      "Average loss at step  936000 :  4.89752142763\n",
      "Average loss at step  938000 :  4.81926231146\n",
      "Average loss at step  940000 :  4.75196073818\n",
      "Average loss at step  942000 :  4.92858184671\n",
      "Average loss at step  944000 :  4.69029008853\n",
      "Average loss at step  946000 :  4.83107067621\n",
      "Average loss at step  948000 :  4.78039385104\n",
      "Average loss at step  950000 :  4.78459160638\n",
      "Average loss at step  952000 :  4.98818264687\n",
      "Average loss at step  954000 :  4.97680851126\n",
      "Average loss at step  956000 :  4.7786157074\n",
      "Average loss at step  958000 :  4.78178973794\n",
      "Average loss at step  960000 :  4.70661386883\n",
      "Average loss at step  962000 :  4.96636486232\n",
      "Average loss at step  964000 :  4.77391627741\n",
      "Average loss at step  966000 :  4.92310619986\n",
      "Average loss at step  968000 :  4.93458359933\n",
      "Average loss at step  970000 :  4.76544683492\n",
      "Average loss at step  972000 :  4.72934217131\n",
      "Average loss at step  974000 :  4.79862518847\n",
      "Average loss at step  976000 :  4.85113820672\n",
      "Average loss at step  978000 :  4.84470614088\n",
      "Average loss at step  980000 :  4.73151209998\n",
      "Average loss at step  982000 :  4.61582961798\n",
      "Average loss at step  984000 :  4.71579477382\n",
      "Average loss at step  986000 :  4.89744535446\n",
      "Average loss at step  988000 :  4.96590464532\n",
      "Average loss at step  990000 :  4.99490608907\n",
      "Average loss at step  992000 :  4.74182099783\n",
      "Average loss at step  994000 :  4.84535537887\n",
      "Average loss at step  996000 :  4.74899201083\n",
      "Average loss at step  998000 :  4.7850033046\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "training time used 2065.2s\n"
     ]
    }
   ],
   "source": [
    "# Step 5. Begin training.\n",
    "start = time.time()\n",
    "if train_mode:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # we must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_labels = generate_batch(batch_size, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs,\n",
    "                         train_labels: batch_labels}\n",
    "\n",
    "            # we perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run())\n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = similarity.eval()  # dimension: (16, 50000)\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = id2word[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "\n",
    "                    # index 1로 시작하는 이유: query 단어는 제외함\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    sim_log_str = ''\n",
    "                    for k in range(top_k):\n",
    "                        close_word = id2word[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                        sim_log_str = '%s %.4f' % (sim_log_str, sim[i, nearest[k]])\n",
    "                    ###print(log_str)\n",
    "                    ###print(sim_log_str)\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(session, \"/tmp/model.ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "    print(\"training time used {:.1f}s\".format(time.time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Step 5-1. query파일을 읽어 target word를 저장한다.\\n\\n###코퍼스에 따라 주석체크###\\nwith open(\"query_words_text8.txt\") as f: #query_words_text8\\n#with open(\"query_words_namu.txt\",encoding=\\'utf-8-sig\\') as f: #query_words_namu\\n    for i in f.readlines(): \\n        line = i.splitlines()[0]\\n        find_word = word2id[line]\\n        find_examples.append(find_word)\\n#print(find_examples)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Step 5-1. query파일을 읽어 target word를 저장한다.\n",
    "\n",
    "###코퍼스에 따라 주석체크###\n",
    "with open(\"query_words_text8.txt\") as f: #query_words_text8\n",
    "#with open(\"query_words_namu.txt\",encoding='utf-8-sig') as f: #query_words_namu\n",
    "    for i in f.readlines(): \n",
    "        line = i.splitlines()[0]\n",
    "        find_word = word2id[line]\n",
    "        find_examples.append(find_word)\n",
    "#print(find_examples)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_examples = []\n",
    "find_word = word2id[\"컴퓨터\"]\n",
    "find_examples.append(find_word)\n",
    "find_word = word2id[\"지배\"]\n",
    "find_examples.append(find_word)\n",
    "find_word = word2id[\"인간\"]\n",
    "find_examples.append(find_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "find_size = len(find_examples)\n",
    "print(find_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored.\n",
      "0.146684\n",
      "0.305355\n",
      "0.382829\n",
      "0.226944\n",
      "0.166694\n"
     ]
    }
   ],
   "source": [
    "# Step 6. Restore checked tf checkpoint, perform query words similarity calculation\n",
    "\n",
    "i = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(session, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # TODO: You can do word similarity calculation by running tensorflow (TF) operation here.\n",
    "    # TODO: You must define a TF operation before running.\n",
    "    # TODO: Hint: See how the 'similarity' TF operation works.\n",
    "    \n",
    "    # Compute the cosine similarity between target examples and all embeddings\n",
    "    \n",
    "    \n",
    "    find_dataset = tf.constant(find_examples, dtype=tf.int32)\n",
    "    find_embeddings = tf.nn.embedding_lookup(final_embeddings, find_dataset)\n",
    "    similarity = tf.matmul(find_embeddings, final_embeddings, transpose_b=True)\n",
    "    \n",
    "    ###코퍼스에 따라 주석체크###    \n",
    "    #f = open(\"result_text8.txt\", 'w') #result_text8\n",
    "    f = open(\"result_namu.txt\", 'w') #result_namu\n",
    "    \n",
    "    sim = similarity.eval()  # dimension: (16, 50000)\n",
    "    for i in range(find_size):\n",
    "        valid_word = id2word[find_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        # index 1로 시작하는 이유: query 단어는 제외함\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        \n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        f.write(valid_word + '\\n')\n",
    "        \n",
    "        \n",
    "        sim_log_str = ''\n",
    "        for k in range(top_k):\n",
    "            close_word = id2word[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "            sim_log_str = '%s %.4f' % (sim_log_str, sim[i, nearest[k]])\n",
    "            output_str = '%.4f %s' % (sim[i, nearest[k]], close_word)\n",
    "            f.write(output_str+'\\n')\n",
    "            \n",
    "        #print(log_str)\n",
    "        #print(sim_log_str)\n",
    "        f.write('\\n')\n",
    "    \n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    #사전에 입력된 3단어(computer, dominate, human)와 similarity를 구한다(text8기반).\n",
    "    print(sim[0,  word2id[\"인공지능\"]]) #computer ----> target: '컴퓨터', '인간', '지배'\n",
    "    print(sim[1,  word2id[\"공격\"]]) #dominate\n",
    "    print(sim[2,  word2id[\"인류\"]]) #human\n",
    "    \n",
    "    print(sim[0,  word2id[\"플라스틱\"]]) #computer\n",
    "    #print(sim[1,  word2id[\"먹다\"]]) #dominate\n",
    "    print(sim[2,  word2id[\"고기\"]]) #human\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.27855176  0.17266934  0.13370354 ...,  0.1603964   0.02334318\n",
      "   0.2540853 ]\n",
      " [ 0.08872499  0.05153894  0.09666556 ...,  0.20880429  0.11884999\n",
      "   0.20902018]\n",
      " [ 0.22076163  0.11755648  0.20945464 ...,  0.20485502  0.0891592\n",
      "   0.37105539]]\n"
     ]
    }
   ],
   "source": [
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11330707  0.20957083  0.05213645 ..., -0.05496944 -0.07511954\n",
      "  -0.01429023]\n",
      " [ 0.02592218  0.12591952  0.01673021 ...,  0.31642792  0.07864259\n",
      "   0.0570894 ]\n",
      " [-0.11569405  0.03058955  0.0162832  ..., -0.00357592 -0.16559833\n",
      "  -0.07483871]\n",
      " ..., \n",
      " [-0.0347858  -0.00723402 -0.17112799 ...,  0.12443602 -0.00114231\n",
      "  -0.00443596]\n",
      " [ 0.06927669 -0.06061516  0.01139778 ...,  0.15416434 -0.09068422\n",
      "   0.04057422]\n",
      " [ 0.0648037   0.0683338  -0.14724544 ...,  0.01714601 -0.10819717\n",
      "  -0.06900365]]\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print( len(final_embeddings) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print( len( final_embeddings[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11330707  0.20957083  0.05213645 -0.20147508  0.06274844 -0.02851136\n",
      " -0.0218526  -0.11967319  0.1144225  -0.06500185  0.06991627 -0.11520065\n",
      "  0.23402511  0.05688337  0.04304961  0.17005458 -0.08168326  0.12907574\n",
      " -0.12438665 -0.00084754  0.11954555 -0.00805354 -0.03922109  0.26986742\n",
      "  0.08278835  0.05652469  0.13816988  0.01116926 -0.00088346  0.03929656\n",
      " -0.09327307  0.03324024  0.01433828 -0.12600112  0.01266643 -0.0754936\n",
      "  0.00213196 -0.00871396  0.0970993  -0.00913507 -0.08878361  0.03871777\n",
      "  0.07194506  0.0186322  -0.05090651  0.11185599 -0.00059311 -0.08514991\n",
      "  0.0233674   0.0284252  -0.02094184  0.00807906  0.1330021  -0.13141136\n",
      "  0.11581417 -0.08027627 -0.02396874  0.07557045 -0.06874256 -0.03601432\n",
      "  0.06927487 -0.11838236  0.07100197 -0.05014864 -0.00465594  0.03391394\n",
      " -0.11283853 -0.17682102  0.01703772 -0.11038593 -0.0100897   0.06629583\n",
      " -0.12584773 -0.03761817 -0.03705151 -0.14996207  0.035885   -0.02342131\n",
      " -0.14551179  0.10678009 -0.02990347 -0.06849657  0.02888504  0.10409715\n",
      "  0.00576606  0.08496403  0.10744922  0.04352443  0.09012853 -0.03162917\n",
      " -0.07985771 -0.02143615  0.04393133 -0.02416357  0.05056344  0.18234175\n",
      "  0.08708338  0.01330384  0.01561459 -0.13480209  0.03937187 -0.0302487\n",
      " -0.12636243 -0.08841264  0.00702537 -0.04827115  0.02436844 -0.03741208\n",
      " -0.11011326 -0.0717483   0.0683117  -0.09128392 -0.06390415 -0.03497591\n",
      "  0.02720422 -0.03022743 -0.10882938  0.01201925  0.22180861 -0.00928146\n",
      "  0.02563637  0.04195141 -0.16018982 -0.0366489  -0.01876868 -0.05496944\n",
      " -0.07511954 -0.01429023]\n"
     ]
    }
   ],
   "source": [
    "print(  final_embeddings[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "零\n"
     ]
    }
   ],
   "source": [
    "print(id2word[49999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999\n"
     ]
    }
   ],
   "source": [
    "print(word2id['零'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
