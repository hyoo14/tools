{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# _*_ coding: UTF-8 _*_\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###코퍼스에 따라 주석체크###\n",
    "corpus_filenm = \"text8.txt\"\n",
    "#corpus_filenm = \"morphs_namu_small.txt\"\n",
    "# corpus_filenm = \"/hdd/data/namu-wiki/morphs_namu_small.txtad\"\n",
    "\n",
    "train_mode = True\n",
    "# train_mode = False\n",
    "\n",
    "batch_size = 128            # Number of examples in a mini-batch\n",
    "embedding_size = 128        # Dimension of the embedding vector.\n",
    "skip_window = 2             # How many words to consider left and right.\n",
    "num_neg_sampled = 64        # Number of negative examples to sample.\n",
    "vocabulary_size = 50000     # Size of vocabulary\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors.\n",
    "valid_size = 16\n",
    "valid_window = 1000\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "###코퍼스에 따라 주석체크###\n",
    "num_steps = 100000 #text8\n",
    "#num_steps = 1000000 #morphs_namu_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Read corpus and tokenize words based on space.\n",
    "def read_data(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "        return ' '.join(lines)\n",
    "\n",
    "text = read_data(corpus_filenm)\n",
    "text_words = text.split()\n",
    "data_size = len(text_words)\n",
    "print('Data size', data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word coverage: 97.54%\n",
      "Most common words (+UNK) [('UNK', 0), ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# Step 2. Build vocabulary using most common top k words, replace word with ids in the corpus.\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"\n",
    "    :param words: list of all words in corpus\n",
    "    :param n_words: vocabulary size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    unique = collections.Counter(words)     # python dict - key(word): value(freq)\n",
    "    orders = unique.most_common(n_words - 1)\n",
    "    count = [('UNK', 0)]\n",
    "    count.extend(orders)\n",
    "\n",
    "    # check vocabulary coverage\n",
    "    total_freq = 0\n",
    "    for word, freq in orders:\n",
    "        total_freq += freq\n",
    "    print(\"word coverage: %.2f%%\" % (100.0 * total_freq / data_size))  # word coverage\n",
    "\n",
    "    # build word2id dictionary and id2word reverse dictionary.\n",
    "    word2id = dict()\n",
    "    for word, _ in count:\n",
    "        word2id[word] = len(word2id)\n",
    "    id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "\n",
    "    # build training data by replacing all words with word ids.\n",
    "    data = list()\n",
    "    for word in words:\n",
    "        # if the word is not in the dictionary, index will be 0. (i.e. 'UNK')\n",
    "        index = word2id.get(word, 0)\n",
    "        data.append(index)\n",
    "\n",
    "    return data, count, word2id, id2word\n",
    "\n",
    "vocab_counts = []\n",
    "data, count, word2id, id2word = build_dataset(text_words, vocabulary_size)\n",
    "\n",
    "\n",
    "del text_words\n",
    "\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [id2word[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3. 학습에 사용할 mini-batch 생성\n",
    "def generate_batch(batch_size, skip_window):\n",
    "    \n",
    "    global data_index\n",
    "    \n",
    "    num_targets = skip_window * 2\n",
    "    \n",
    "    assert batch_size % num_targets == 0\n",
    "    assert num_targets <= 2 * skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape=batch_size, dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 \n",
    "    buffer = collections.deque(maxlen=span)##########buffer에 문제가 있음 deque error가 계속 뜬다..\n",
    "    \n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "        \n",
    "        \n",
    "    #buffer.append(data[data_index: data_index + span])\n",
    "    #data_index += span\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "    \n",
    "    for i in range(batch_size // num_targets):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "\n",
    "        for j, context_word in enumerate(context_words):\n",
    "            batch[i * num_targets + j ] = buffer[skip_window]#buffer[0][skip_window]        # center words  e.g. [2, 2, 2, 2]   #ㄹㅇ..?\n",
    "            labels[i * num_targets + j, 0] = buffer[context_word]#buffer[0][context_word]   # context words e.g. [0, 1, 3, 4]\n",
    "\n",
    "        if data_index == len(data):\n",
    "            # reset data index\n",
    "            for word in data[:span]:\n",
    "                buffer.append(word)\n",
    "        else:\n",
    "            # adding words to buffer\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 as -> 5234 anarchism\n",
      "12 as -> 3081 originated\n",
      "12 as -> 6 a\n",
      "12 as -> 195 term\n",
      "6 a -> 3081 originated\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "6 a -> 2 of\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "batch, labels = generate_batch(batch_size=8, skip_window=skip_window)\n",
    "for i in range(8):\n",
    "    print(batch[i], id2word[batch[i]],\n",
    "          '->', labels[i, 0], id2word[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity :  Tensor(\"MatMul_1:0\", shape=(16, 50000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build a skip-gram tensorflow graph.\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)         # word_ids of validation words\n",
    "\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Important - convert word ids to embedding vectors\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss - NCE: Noise Contrasive Estimation\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    # http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_neg_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer: set up a variable that's incremented once per batch and controls the learning rate decay.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.1,            # Base learning rate.\n",
    "        global_step,    # Current index into the dataset.\n",
    "        num_steps,      # Decay step.\n",
    "        0.95,           # Decay rate.\n",
    "        staircase=True)\n",
    "\n",
    "    # Use simple momentum for the optimization.\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Compute the cosine similarity between mini-batch examples and all embeddings\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "\n",
    "    # validation dataset의 유사 단어 찾기\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    print('similarity : ',similarity)\n",
    "    # Add variable initializer\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  281.556549072\n",
      "Average loss at step  2000 :  149.14271468\n",
      "Average loss at step  4000 :  83.2791191893\n",
      "Average loss at step  6000 :  60.4220091286\n",
      "Average loss at step  8000 :  46.0132020755\n",
      "Average loss at step  10000 :  36.1916462045\n",
      "Average loss at step  12000 :  32.7799281759\n",
      "Average loss at step  14000 :  27.3139249136\n",
      "Average loss at step  16000 :  24.288658298\n",
      "Average loss at step  18000 :  21.942700624\n",
      "Average loss at step  20000 :  19.0564662199\n",
      "Average loss at step  22000 :  17.7507429862\n",
      "Average loss at step  24000 :  15.6627304223\n",
      "Average loss at step  26000 :  15.563278006\n",
      "Average loss at step  28000 :  14.4819079757\n",
      "Average loss at step  30000 :  13.0619926206\n",
      "Average loss at step  32000 :  12.2425263586\n",
      "Average loss at step  34000 :  11.3232801001\n",
      "Average loss at step  36000 :  11.0874902821\n",
      "Average loss at step  38000 :  11.2453028466\n",
      "Average loss at step  40000 :  10.597443682\n",
      "Average loss at step  42000 :  9.63298297811\n",
      "Average loss at step  44000 :  9.40682151723\n",
      "Average loss at step  46000 :  9.97308383095\n",
      "Average loss at step  48000 :  9.16495975077\n",
      "Average loss at step  50000 :  9.52397625792\n",
      "Average loss at step  52000 :  9.1761050837\n",
      "Average loss at step  54000 :  8.98024982274\n",
      "Average loss at step  56000 :  8.87963411736\n",
      "Average loss at step  58000 :  8.05324457598\n",
      "Average loss at step  60000 :  8.50144088638\n",
      "Average loss at step  62000 :  8.52610810399\n",
      "Average loss at step  64000 :  8.03800320548\n",
      "Average loss at step  66000 :  7.99746999145\n",
      "Average loss at step  68000 :  7.58841424704\n",
      "Average loss at step  70000 :  8.25011867106\n",
      "Average loss at step  72000 :  7.71167665648\n",
      "Average loss at step  74000 :  7.89199382663\n",
      "Average loss at step  76000 :  7.34769946861\n",
      "Average loss at step  78000 :  6.93654310369\n",
      "Average loss at step  80000 :  7.5497212044\n",
      "Average loss at step  82000 :  7.37378430855\n",
      "Average loss at step  84000 :  7.15400654078\n",
      "Average loss at step  86000 :  7.33809415364\n",
      "Average loss at step  88000 :  7.11082148051\n",
      "Average loss at step  90000 :  7.20803089988\n",
      "Average loss at step  92000 :  6.98283713174\n",
      "Average loss at step  94000 :  6.97708833826\n",
      "Average loss at step  96000 :  7.11833304656\n",
      "Average loss at step  98000 :  6.60002548945\n",
      "Model saved in path: /tmp/model.ckpt\n",
      "training time used 341.6s\n"
     ]
    }
   ],
   "source": [
    "# Step 5. Begin training.\n",
    "start = time.time()\n",
    "if train_mode:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # we must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_labels = generate_batch(batch_size, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs,\n",
    "                         train_labels: batch_labels}\n",
    "\n",
    "            # we perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run())\n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = similarity.eval()  # dimension: (16, 50000)\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = id2word[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "\n",
    "                    # index 1로 시작하는 이유: query 단어는 제외함\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    sim_log_str = ''\n",
    "                    for k in range(top_k):\n",
    "                        close_word = id2word[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                        sim_log_str = '%s %.4f' % (sim_log_str, sim[i, nearest[k]])\n",
    "                    ###print(log_str)\n",
    "                    ###print(sim_log_str)\n",
    "\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(session, \"/tmp/model.ckpt\")\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "    print(\"training time used {:.1f}s\".format(time.time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"# Step 5-1. query파일을 읽어 target word를 저장한다.\n",
    "\n",
    "###코퍼스에 따라 주석체크###\n",
    "with open(\"query_words_text8.txt\") as f: #query_words_text8\n",
    "#with open(\"query_words_namu.txt\",encoding='utf-8-sig') as f: #query_words_namu\n",
    "    for i in f.readlines(): \n",
    "        line = i.splitlines()[0]\n",
    "        find_word = word2id[line]\n",
    "        find_examples.append(find_word)\n",
    "#print(find_examples)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "find_examples = []\n",
    "find_word = word2id[\"computer\"]\n",
    "find_examples.append(find_word)\n",
    "find_word = word2id[\"dominate\"]\n",
    "find_examples.append(find_word)\n",
    "find_word = word2id[\"human\"]\n",
    "find_examples.append(find_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "find_size = len(find_examples)\n",
    "print(find_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored.\n",
      "0.0637395\n",
      "0.673611\n",
      "0.129732\n",
      "0.200213\n",
      "0.610277\n",
      "0.172734\n"
     ]
    }
   ],
   "source": [
    "# Step 6. Restore checked tf checkpoint, perform query words similarity calculation\n",
    "\n",
    "i = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(session, \"/tmp/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "    # TODO: You can do word similarity calculation by running tensorflow (TF) operation here.\n",
    "    # TODO: You must define a TF operation before running.\n",
    "    # TODO: Hint: See how the 'similarity' TF operation works.\n",
    "    \n",
    "    # Compute the cosine similarity between target examples and all embeddings\n",
    "    \n",
    "    \n",
    "    find_dataset = tf.constant(find_examples, dtype=tf.int32)\n",
    "    find_embeddings = tf.nn.embedding_lookup(final_embeddings, find_dataset)\n",
    "    similarity = tf.matmul(find_embeddings, final_embeddings, transpose_b=True)\n",
    "    \n",
    "    ###코퍼스에 따라 주석체크###    \n",
    "    f = open(\"result_text8.txt\", 'w') #result_text8\n",
    "    #f = open(\"result_namu.txt\", 'w') #result_namu\n",
    "    \n",
    "    sim = similarity.eval()  # dimension: (16, 50000)\n",
    "    for i in range(find_size):\n",
    "        valid_word = id2word[find_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        # index 1로 시작하는 이유: query 단어는 제외함\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        \n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        f.write(valid_word + '\\n')\n",
    "        \n",
    "        \n",
    "        sim_log_str = ''\n",
    "        for k in range(top_k):\n",
    "            close_word = id2word[nearest[k]]\n",
    "            log_str = '%s %s,' % (log_str, close_word)\n",
    "            sim_log_str = '%s %.4f' % (sim_log_str, sim[i, nearest[k]])\n",
    "            output_str = '%.4f %s' % (sim[i, nearest[k]], close_word)\n",
    "            f.write(output_str+'\\n')\n",
    "            \n",
    "        #print(log_str)\n",
    "        #print(sim_log_str)\n",
    "        f.write('\\n')\n",
    "    \n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    #사전에 입력된 3단어(computer, dominate, human)와 similarity를 구한다(text8기반).\n",
    "    print(sim[0,  word2id[\"ai\"]]) #computer\n",
    "    print(sim[1,  word2id[\"conquer\"]]) #dominate\n",
    "    print(sim[2,  word2id[\"mankind\"]]) #human\n",
    "    \n",
    "    print(sim[0,  word2id[\"plastic\"]]) #computer\n",
    "    print(sim[1,  word2id[\"eat\"]]) #dominate\n",
    "    print(sim[2,  word2id[\"meat\"]]) #human\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing word vectors into file..\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# You can save word vectors into files like this.\n",
    "embedding_filenm = \"word2vec.txt\"\n",
    "print(\"Writing word vectors into file..\")\n",
    "with open(embedding_filenm, \"w\") as f:\n",
    "    for word, vec in zip(word2id.keys(), final_embeddings):\n",
    "        out = word + ' ' + ' '.join([str(v) for v in list(vec)]) + \"\\n\"\n",
    "        f.write(out)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
