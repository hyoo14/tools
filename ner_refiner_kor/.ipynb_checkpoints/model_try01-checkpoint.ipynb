{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shared global variables to be imported from model also\n",
    "UNK = \"$UNK$\"\n",
    "NUM = \"$NUM$\"\n",
    "NONE = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatches(data, minibatch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: generator of (sentence, tags) tuples\n",
    "        minibatch_size: (int)\n",
    "\n",
    "    Yields:\n",
    "        list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = [], []\n",
    "    for (x, y) in data:\n",
    "        if len(x_batch) == minibatch_size:\n",
    "            yield x_batch, y_batch\n",
    "            x_batch, y_batch = [], []\n",
    "\n",
    "        if type(x[0]) == tuple:\n",
    "            x = zip(*x)\n",
    "        x_batch += [x]\n",
    "        y_batch += [y]\n",
    "\n",
    "    if len(x_batch) != 0:\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "    \"\"\"\n",
    "    sequence_padded, sequence_length = [], []\n",
    "\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok]*max(max_length - len(seq), 0)\n",
    "        sequence_padded +=  [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, pad_tok, nlevels=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sequences: a generator of list or tuple\n",
    "        pad_tok: the char to pad with\n",
    "        nlevels: \"depth\" of padding, for the case where we have characters ids\n",
    "\n",
    "    Returns:\n",
    "        a list of list where each sublist has same length\n",
    "\n",
    "    \"\"\"\n",
    "    if nlevels == 1:\n",
    "        max_length = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, sequence_length = _pad_sequences(sequences,\n",
    "                                            pad_tok, max_length)\n",
    "\n",
    "    elif nlevels == 2:\n",
    "        max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                               for seq in sequences])\n",
    "        sequence_padded, sequence_length = [], []\n",
    "        for seq in sequences:\n",
    "            # all words are same length now\n",
    "            sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "            sequence_padded += [sp]\n",
    "            sequence_length += [sl]\n",
    "\n",
    "        max_length_sentence = max(map(lambda x : len(x), sequences))\n",
    "        sequence_padded, _ = _pad_sequences(sequence_padded,\n",
    "                [pad_tok]*max_length_word, max_length_sentence)\n",
    "        sequence_length, _ = _pad_sequences(sequence_length, 0,\n",
    "                max_length_sentence)\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chunks(seq, tags):\n",
    "    \"\"\"Given a sequence of tags, group entities and their position\n",
    "\n",
    "    Args:\n",
    "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Returns:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Example:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "    default = tags[NONE]\n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    chunks = []\n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        # End of a chunk 1\n",
    "        if tok == default and chunk_type is not None:\n",
    "            # Add a chunk.\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        # End of a chunk + start of a chunk!\n",
    "        elif tok != default:\n",
    "\n",
    "            tag_name = idx_to_tag[tok]\n",
    "            tok_chunk_class = tag_name.split('-')[0]\n",
    "            tok_chunk_type = tag_name.split('-')[-1]\n",
    "\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # end condition\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERmodel(object):\n",
    "    \"\"\"Generic class for general methods that are not specific to NER\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Defines self.config and self.logger\n",
    "\n",
    "        Args:\n",
    "            config: (Config instance) class with hyper parameters,\n",
    "                vocab and embeddings\n",
    "\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.sess   = None\n",
    "        self.saver  = None\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in\n",
    "                           self.config.vocab_tags.items()}\n",
    "\n",
    "    def build(self):\n",
    "        #Step1\n",
    "        self.sequence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sequence_lengths\")\n",
    "        self.labels = tf.placeholder(tf.int32, shape=[None, None], name=\"labels\")\n",
    "        self.dropout = tf.placeholder(dtype=tf.int32, shape=[], name=\"dropout\")\n",
    "        self.lr = tf.placeholder(dtype=tf.int32, shape=[], name=\"lr\")\n",
    "        self.word_ids = tf.placeholder(tf.int32, shape=[None, None], name=\"word_ids\")\n",
    "        \n",
    "        #Step2\n",
    "        with tf.variable_scope(\"words\"):\n",
    "            \n",
    "            if self.config.use_pretrained is False:\n",
    "                print(\"Randomly initializing word vectors\")\n",
    "                _word_embeddings = tf.get_variable(\n",
    "                name = \"_word_embeddings\",\n",
    "                dtype = tf.float32,\n",
    "                shape = [self.config.nwords, self.config.dim_word])\n",
    "            \n",
    "            else:\n",
    "                print(\"Using pre-trained word vectors : \"+sewlf.config.filename_embedding)\n",
    "                _word_embeddings = tf.Variabel(\n",
    "                self.config.embeddings,\n",
    "                name = \"_word_embeddings\",\n",
    "                dtype = tf.float32)\n",
    "            \n",
    "            word_embeddings = tf.nn.embedding_sookup(_word_embeddings,\n",
    "                                                    self.word_ids, name=\"word_embeddings\")\n",
    "        self.word_embeddings = word_embeddings\n",
    "        \n",
    "        #step3\n",
    "        cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "        cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm)\n",
    "        (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw, cell_bw, self.word_embeddings,\n",
    "        sequence_length = self.sequence_lengths, dtype=tf.float32)\n",
    "        \n",
    "        output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "        output = tf.nn.dropout(output, self.dropout)\n",
    "        \n",
    "        nsteps = tf.shape(output)\n",
    "        \n",
    "        #Step4\n",
    "        output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm])\n",
    "        W = tf.get_variable(\"W\", dtype=tf.float32,\n",
    "                           shape = [2*self.config.hidden_size_lstm, self.config.ntages])\n",
    "        b = tf.get_variable(\"b\", dtype=tf.float32,\n",
    "                           shape = [self.config.ntages], initializer = tf.zeros_initializer())\n",
    "        pred = tf.matmul(output, W) + b\n",
    "        \n",
    "        self.logits = tf.reshape(pred, [-1, nsteps, self.config,ntags])\n",
    "        \n",
    "        #Step5\n",
    "        losses = tf.nn.sparse_softmax_corss_entropy_with_logits(\n",
    "        logits=self.logits, labels = self.labels)\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "        self.labels_pred = tf.cast(tf.argmax(self.logits, axis=-1), tf.int32)\n",
    "        \n",
    "        #Step6\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "\n",
    "    def train(self, train, dev):\n",
    "        \"\"\"Performs training with early stopping and lr exponential decay\n",
    "\n",
    "        Args:\n",
    "            train: dataset that yields tuple of (sentences, tags)\n",
    "            dev: dataset\n",
    "\n",
    "        \"\"\"\n",
    "        best_score = 0\n",
    "        nepoch_no_imprv = 0 # for early stopping\n",
    "\n",
    "        for epoch in range(self.config.nepochs):\n",
    "            print(\"Epoch {:} out of {:}\".format(epoch + 1, self.config.nepochs))\n",
    "\n",
    "            #score = self.run_epoch(train, dev, epoch)\n",
    "\n",
    "            #############3\n",
    "            batch_size = self.config.batch_size\n",
    "            #nbatches = (len(train) + batch_size - 1)\n",
    "\n",
    "            # iterate over dataset\n",
    "            for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
    "                fd, _ = self.get_feed_dict(words, labels, self.config.lr,\n",
    "                                           self.config.dropout)\n",
    "\n",
    "                _, train_loss = self.sess.run(\n",
    "                    [self.train_op, self.loss], feed_dict=fd)\n",
    "\n",
    "                # print(i + 1, [(\"train loss\", train_loss)])\n",
    "\n",
    "            metrics = self.run_evaluate(dev)\n",
    "            print(\"acc : \" + str('%.2f' % metrics['acc']) + \" - \" + \"f1 : \" + str('%.2f' % metrics['f1']))\n",
    "\n",
    "            score = metrics[\"f1\"]\n",
    "            ########## ju_edit\n",
    "            self.config.lr *= self.config.lr_decay # decay learning rate\n",
    "\n",
    "            # early stopping and saving best parameters\n",
    "            if score >= best_score:\n",
    "                nepoch_no_imprv = 0\n",
    "                if not os.path.exists(self.config.dir_model):\n",
    "                    os.makedirs(self.config.dir_model)\n",
    "                self.saver.save(self.sess, self.config.dir_model)\n",
    "                best_score = score\n",
    "                print (\"new best score\")\n",
    "            else:\n",
    "                nepoch_no_imprv += 1\n",
    "                if nepoch_no_imprv >= self.config.nepoch_no_imprv:\n",
    "                    print (\"- early stopping {} epochs without \"\"improvement\".format(nepoch_no_imprv))\n",
    "                    break\n",
    "    \n",
    "    def restore_session(self, dir_model):\n",
    "        \"\"\"Reload weights into session\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            dir_model: dir with weights\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"Reloading the latest trained model...\")\n",
    "        self.saver.restore(self.sess, dir_model)\n",
    "\n",
    "    def run_evaluate(self, test):\n",
    "\n",
    "        \"\"\"Evaluates performance on test set\n",
    "\n",
    "        Args:\n",
    "            test: dataset that yields tuple of (sentences, tags)\n",
    "\n",
    "        Returns:\n",
    "            metrics: (dict) metrics[\"acc\"] = 98.4, ...\n",
    "\n",
    "        \"\"\"\n",
    "        accs = []\n",
    "        correct_preds, total_correct, total_preds = 0., 0., 0.\n",
    "        for words, labels in minibatches(test, self.config.batch_size):\n",
    "            labels_pred, sequence_lengths = self.predict_batch(words)\n",
    "\n",
    "            for lab, lab_pred, length in zip(labels, labels_pred,\n",
    "                                             sequence_lengths):\n",
    "                lab = lab[:length]\n",
    "                lab_pred = lab_pred[:length]\n",
    "                accs += [a == b for (a, b) in zip(lab, lab_pred)]\n",
    "\n",
    "                lab_chunks = set(get_chunks(lab, self.config.vocab_tags))\n",
    "                lab_pred_chunks = set(get_chunks(lab_pred,\n",
    "                                                 self.config.vocab_tags))\n",
    "\n",
    "                correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "                total_preds += len(lab_pred_chunks)\n",
    "                total_correct += len(lab_chunks)\n",
    "\n",
    "        p = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "        r = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "        acc = np.mean(accs)\n",
    "\n",
    "        return {\"acc\": 100 * acc, \"f1\": 100 * f1}\n",
    "\n",
    "    def get_feed_dict(self, words, labels=None, lr=None, dropout=None):\n",
    "        \"\"\"Given some data, pad it and build a feed dictionary\n",
    "\n",
    "        Args:\n",
    "            words: list of sentences. A sentence is a list of ids of a list of\n",
    "                words. A word is a list of ids\n",
    "            labels: list of ids\n",
    "            lr: (float) learning rate\n",
    "            dropout: (float) keep prob\n",
    "\n",
    "        Returns:\n",
    "            dict {placeholder: value}\n",
    "\n",
    "        \"\"\"\n",
    "        # perform padding of the given data\n",
    "\n",
    "        word_ids, sequence_lengths = pad_sequences(words, 0)\n",
    "\n",
    "        # build feed dictionary\n",
    "        feed = {\n",
    "            self.word_ids: word_ids,\n",
    "            self.sequence_lengths: sequence_lengths\n",
    "        }\n",
    "\n",
    "        if labels is not None:\n",
    "            labels, _ = pad_sequences(labels, 0)\n",
    "            feed[self.labels] = labels\n",
    "\n",
    "        if lr is not None:\n",
    "            feed[self.lr] = lr\n",
    "\n",
    "        if dropout is not None:\n",
    "            feed[self.dropout] = dropout\n",
    "\n",
    "        return feed, sequence_lengths\n",
    "\n",
    "    def predict_batch(self, words):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            words: list of sentences\n",
    "\n",
    "        Returns:\n",
    "            labels_pred: list of labels for each sentence\n",
    "            sequence_length\n",
    "\n",
    "        \"\"\"\n",
    "        fd, sequence_lengths = self.get_feed_dict(words, dropout=1.0)\n",
    "\n",
    "        labels_pred = self.sess.run(self.labels_pred, feed_dict=fd)\n",
    "\n",
    "        return labels_pred, sequence_lengths\n",
    "\n",
    "    def predict(self, words_raw):\n",
    "        \"\"\"Returns list of tags\n",
    "\n",
    "        Args:\n",
    "            words_raw: list of words (string), just one sentence (no batch)\n",
    "\n",
    "        Returns:\n",
    "            preds: list of tags (string), one for each word in the sentence\n",
    "\n",
    "        \"\"\"\n",
    "        words = [self.config.processing_word(w) for w in words_raw]\n",
    "        if type(words[0]) == tuple:\n",
    "            words = zip(*words)\n",
    "        pred_ids, _ = self.predict_batch([words])\n",
    "        preds = [self.idx_to_tag[idx] for idx in list(pred_ids[0])]\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
